{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XHjvFZ7QjBo"
      },
      "outputs": [],
      "source": [
        "# 데이터 수집\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_info = pd.read_csv('info_dataset/info.xlsx', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_0 = pd.read_csv('review_dataset/reveiw0.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_1 = pd.read_csv('review_dataset/reveiw1.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_2 = pd.read_csv('review_dataset/reveiw2.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_3 = pd.read_csv('review_dataset/reveiw3.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_4 = pd.read_csv('review_dataset/reveiw4.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_5 = pd.read_csv('review_dataset/reveiw5.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_6 = pd.read_csv('review_dataset/reveiw6.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "df_7 = pd.read_csv('review_dataset/reveiw7.csv', encoding='utf8', index_col=0, errors='ignore'\n",
        "df_8 = pd.read_csv('review_dataset/reveiw8.csv', encoding='utf8', index_col=0, errors='ignore')\n",
        "\n",
        "df_0.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링 방법\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "## hotel_review_list.csv 를 읽어와서 데이터프레임을 만드는 함수\n",
        "def make_df(url_list, num) :\n",
        "\n",
        "    columns = url_list.iloc[:,num].dropna()\n",
        "    \n",
        "    for idx,column in enumerate(columns) :\n",
        "        columns[idx] = column.split('/')[-1] # url의 고유 ID 가져오기\n",
        "\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "## 소스를 받으면 리뷰 내용들을 찾아서 하나의 문자열로 합친 후 반환하는 함수\n",
        "def get_reviews(src, df, col) : # src : 소스 , df : 저장될 데이터 프레임, col : 호텔 ID 컬럼\n",
        "    # print(df)\n",
        "    col_name = df.columns[col]\n",
        "    idx = 0\n",
        "    soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "    review_list_short = soup.find_all('p',attrs={\"class\":\"content\"})        # 짧은 리뷰 리스트\n",
        "    review_list_long = soup.find_all('p',attrs={\"class\":\"content clamp\"})   # 더보기 리뷰 리스트\n",
        "\n",
        "    # 리뷰를 df에 저장 \n",
        "    print('짧은 리뷰 저장 중')\n",
        "    for i in range(len(review_list_short)) :     \n",
        "        df.loc[idx, col_name] = review_list_short[i].get_text()\n",
        "        idx += 1\n",
        "        # print('.',end='')\n",
        "\n",
        "    print('\\n긴 리뷰 저장 중')\n",
        "    for i in range(len(review_list_long)) :\n",
        "        df.loc[idx, col_name] = review_list_long[i].get_text()   \n",
        "        idx += 1\n",
        "        # print('.',end='')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "I120wA1uQjb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰 추출\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "import time\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "import method       # method.py가 같은 폴더에 있어야 한다.\n",
        "\n",
        "## 여기 이전에 했던 거처럼 0~8번 자동으로 넣어주게 해도 좋아요. 이거는 숫자 일일이 넣어야 됩니다.\n",
        "num = int(input('input number : '))\n",
        "\n",
        "# url csv 파일 읽어오는 부분\n",
        "url_list = pd.read_csv('hotel_review_list.csv', index_col = 0)\n",
        "\n",
        "# 각 컬럼의 길이 구하기\n",
        "numofhotel = url_list.iloc[:,num].notnull().sum()\n",
        "\n",
        "# 내보낼 df 생성 (row : 리뷰, col : ID)\n",
        "# ID : 호텔 url 맨 뒤 숫자부분\n",
        "df = method.make_df(url_list, num)\n",
        "\n",
        "# 입력된 column 의 url을 훑으며 크롤링 시작\n",
        "for idx in range(numofhotel) :\n",
        "\n",
        "    # 진행상황 출력\n",
        "    print('-'*60)\n",
        "    print(f'read page {idx}/{numofhotel}')\n",
        "\n",
        "    # '/reviews' 를 붙여 url 완성\n",
        "    url = url_list.iloc[idx,num] + '/reviews' \n",
        "    print(f'url : {url}') \n",
        "\n",
        "    # chromedriver 옵션 설정\n",
        "    options = Options()\n",
        "    user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.83 Mobile Safari/537.36\"\n",
        "    options.add_argument('user-agent=' + user_agent)    # 유저의 접속인 것처럼 속이기 \n",
        "    options.add_argument(\"headless\")        # 창 띄우고 싶으면 주석처리.    \n",
        "\n",
        "    # chromedriver 실행 후 url 정보 받기\n",
        "    driver = webdriver.Chrome(options=options, executable_path='chromedriver')\n",
        "    driver.get(url = url)\n",
        "\n",
        "    # 스크롤 맨 밑까지 내리는 작업\n",
        "    print('스크롤 내리는 중')\n",
        "    while True :\n",
        "        print('.',end='')\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "       \n",
        "        time.sleep(2)        ## 스크롤 내리는 로딩시간이 길면 숫자 좀 늘려주세요!\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")        \n",
        "\n",
        "        if new_height == last_height :\n",
        "            break        \n",
        "    print('')\n",
        "\n",
        "    # 크롤링 시작\n",
        "    print('start crawling...')\n",
        "    src = driver.page_source    # 소스를 가져와서 리뷰 데이터 긁어올 것임    \n",
        "    \n",
        "    # 데이터프레임에 크롤링 정보 저장\n",
        "    df = method.get_reviews(src, df, idx) # method 파일 참조하여 리뷰 수집\n",
        "    print('done!')\n",
        "\n",
        "    ## tmp\n",
        "    if idx % 10 == 1 :\n",
        "        df.to_csv(f'./reveiw_list{num}.csv', encoding=\"utf-8-sig\")    \n",
        "        print('tmp saved')    \n",
        "\n",
        "# 모든 작업 종료 후 파일 내보내기\n",
        "df.to_csv(f'./reveiw_list{num}.csv', encoding=\"utf-8-sig\")\n",
        "print('Complete!!')"
      ],
      "metadata": {
        "id": "0TkjyXe7Qje0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 정제\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "num = 0\n",
        "for num in range(9) :\n",
        "\n",
        "    raw_df = pd.read_csv(f'review_dataset/reveiw_list{num}.csv',index_col=0)\n",
        "    new_df = pd.DataFrame(columns=['hotel','star','review'])\n",
        "\n",
        "    ids = raw_df.columns\n",
        "    for id in ids :    \n",
        "        reviews = raw_df.loc[:,id]\n",
        "\n",
        "        for review in reviews :\n",
        "            if pd.isnull(review) :\n",
        "                break\n",
        "\n",
        "            new_df.loc[len(new_df), 'hotel'] = id\n",
        "            new_df.loc[len(new_df) - 1, 'review'] = review\n",
        "            \n",
        "        print(f'{num}번 파일 id : {id} done!')\n",
        "\n",
        "    new_df.to_csv(f'./revised_review{num}.csv', encoding=\"utf-8-sig\")\n",
        "    print(f'reveiw_list{num}.csv DONE!' )\n",
        "    print('='*50)"
      ],
      "metadata": {
        "id": "Jb9vCxlGQjhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLLOPsawRLih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfbn_v2JRLk1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}